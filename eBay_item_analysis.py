# -*- coding: utf-8 -*-
"""DDR_Midterm_MaxineLI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NVgy8_vsx_mqmcXVoXuRN0Dh0LcKCBiq

## Web-scraping: Which sellers advertise/sponsor on eBay
"""

import requests, pandas as pd, re, os, pymysql
from bs4 import BeautifulSoup
from time import sleep

# fetches eBay's search result page for "playstation 4 slim", with each page 100 items
# get the sponsered links
base_url = 'https://www.ebay.com/sch/i.html?_from=R40&_nkw=playstation+4+slim&_sacat=0&LH_TitleDesc=0&_ipg=100&_pgn={}'
seed_urls = [base_url.format(str(i)) for i in range(1,11)] 

title = []
link = []

for i,url in enumerate(seed_urls):
    i+=1
    data = requests.get(url)
    
    soup = BeautifulSoup(data.text, 'html.parser')        
    temp1 = soup.find_all('h3', class_ = 's-item__title s-item__title--has-tags')
    for j in range(len(temp1)):    
        title.append(re.sub('<span(.*?)>', "", str(temp1[j].contents[0])))
        temp2 = temp1[j].find_parents('a','s-item__link')
        link.append(re.findall(r'href="(.*?)"{1}', str(temp2))[0].split('?')[0])
        
# store the sponsered links         
spon_link = link.copy()
with open('sponsored.txt','w') as file:
    for i in spon_link:    
        file.write(i+'\n')
    file.close()

# get the nonsponsered links
title = []
link = []

for i,url in enumerate(seed_urls):
    i+=1
    data = requests.get(url)
    
    soup = BeautifulSoup(data.text, 'html.parser')        
    temp1 = soup.find_all('h3', class_ = 's-item__title')
    for j in range(len(temp1)):    
        title.append(re.sub('<span(.*?)>', "", str(temp1[j].contents[0])))
        temp2 = temp1[j].find_parents('a','s-item__link')
        link.append(re.findall(r'href="(.*?)"{1}', str(temp2))[0].split('?')[0])

# store the nonsponsered links 
nonspon_link = link.copy()
actual_nonspon_link = [a for a in nonspon_link if a not in spon_link]
with open('nonsponsored.txt','w') as file:
    for i in actual_nonspon_link:    
        file.write(i+'\n')
    file.close()

# Create two folders and name them "sponsored" and "non-sponsored" in the same directory.
if not os.path.exists('sponsored'):
    os.makedirs('sponsored')
if not os.path.exists('non-sponsored'):
    os.makedirs('non-sponsored')

# print pages to sponsored file 
headers = {'User-Agent': 'Mozilla/5.0'}
data = []
filename1 = []
with open('sponsored.txt','r') as file:
    for i in file:
        data = requests.get(i.strip(),headers = headers)
        # sleep(2)
        id_num = i.split('/')[-1].strip()
        file_name = id_num+'.htm'
        filename1.append(file_name)
        page = open(f'sponsored/{file_name}', "w")
        page.write(data.text)
        page.close()

# print pages to non-sponsored file
data = []
filename2 = []
with open('nonsponsored.txt','r') as file:
    for i in file:
        data = requests.get(i.strip(),headers = headers)
        # sleep(2)
        id_num = i.split('/')[-1].strip()
        file_name = id_num+'.htm'
        filename2.append(file_name)
        page = open(f'non-sponsored/{file_name}', "w")
        page.write(data.text)
        page.close()

# create file name variable to read the file in a loop
files = [a for a in os.listdir('sponsored') if a.endswith('.htm')]
files= (files + [a for a in os.listdir('non-sponsored') if a.endswith('.htm')])

# read the file, scrap items in a loop and store the output
final_results = []
for i in files:
    output = {}
    # print(i)
    if i in os.listdir('sponsored'):
        file = open(f'sponsored/{i}', "r")
        sponsored = True
    else:
        file = open(f'non-sponsored/{i}', "r")
        sponsored = False
    soup = BeautifulSoup(file.read(), 'html.parser') 
    try: # skip some expired links
        name = soup.find('span', class_ = 'mbg-nw').text.strip() 
    
        score = soup.find('span', class_ = 'mbg-l').find('a').text.strip() # upper level "span" tag has more stable parameter than tag "a"
        price = soup.find('span', itemprop = 'price')
        if price:
            price = int(float(re.findall('[0-9]+.[0-9]+', price.text.strip())[0])*100) # transfer into a "dollar-cent" format
        else:
            price = None

        title = soup.find('h1', id = 'itemTitle').text.replace('Details about','').strip() # use replace to strip the unnecessary string
        cond = soup.find('div', class_ = 'u-flL condText').text.strip()             

        summary = soup.find('span', id = 'shSummary') # There are three categories of shipping prices
        if 'free' in summary.text.lower():
            shipprc = 0
        elif 'calculate' in summary.text.lower():
            shipprc = None
        else:
            shipcost = summary.find('span', id = 'fshippingCost')
            if shipcost:
                shipprc = shipcost.find('span').text
                shipprc = int(float(re.findall('[0-9]+.[0-9]+', shipprc)[0])*100) # transfer into a "dollar-cent" format
            else:
                shipprc = None 
                

        num_text = soup.find('a', class_ = 'vi-txt-underline') # use regex to extract only the number 
        if num_text is not None:
            num = ''.join(re.findall('[0-9]+', num_text.text.strip()))
        else:
            num = 0

        bestofr = soup.find('div', id = 'bstofr')
        if bestofr:
            bestofr_available = 1
        else:
            bestofr_available = 0


        if 'No' in soup.find('div', id = 'why2buy').text:
            retpc = 0
        else: 
            retpc = 1

    except:
        continue
    
    output['seller_name'] = name
    output['seller_score'] = score 
    output['item_price'] = price
    output['num_item'] = num            
    output['best_offer'] = bestofr_available
    output['title'] = title
    output['retpc'] = retpc
    output['shipprc'] = shipprc
    output['condition'] = cond
    output['sponsored'] = sponsored
    output['prodID'] = int(i.split('.')[0])
    # at page end append to main list
    final_results.append(output)

df = pd.DataFrame(final_results)

df.dtypes

# transform data types
df['item_price'] = df['item_price'].astype('Int64')
df['shipprc'] = df['shipprc'].astype('Int64')
df['seller_score'] = df['seller_score'].astype(int)
df['num_item'] = df['num_item'].astype(int)
df['retpc'] = df['retpc'].astype(int)

df.head() # before transfer data into MySQL, first get the stats information

# connect to SQL server and create database and empty table
try:

     #connect to server
    conn = pymysql.connect(host='localhost', user = 'root', password = '54Dreamer1015')
    cursor = conn.cursor()

    query = "CREATE DATABASE IF NOT EXISTS ebay ;"
    print(query)
    cursor.execute(query)
    conn.commit()
        
    query = "CREATE TABLE IF NOT EXISTS ebay.ebay_items \
    (prodID BIGINT, sponsored BOOL, best_offer BOOL, `condition` VARCHAR(50), item_price INT(10), \
    num_item INT(10), retpc VARCHAR(50), seller_name VARCHAR(50), \
    seller_score INT(10), shipprc INT(10), title VARCHAR(200));"
    print(query)
    cursor.execute(query)
    conn.commit()
    cursor.close()
    conn.close()

except IOError as e:
    print(e)
    
cursor.close()

# save the information of items in (d) into a single table
conn = pymysql.connect(host='localhost', user = 'root', password = '54Dreamer1015')
cursor = conn.cursor()

for df in final_results:
    insert_query='insert into ebay.ebay_items values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)'
    
    cursor.execute(insert_query,[df['prodID'], df['sponsored'], df['best_offer'],\
    df['condition'],\
    df['item_price'],\
    df['num_item'],\
    df['retpc'],\
    df['seller_name'],\
    df['seller_score'],\
    df['shipprc'],\
    df['title']])
conn.commit()
cursor.close()

# summary stats on each item from the SQL database
conn = pymysql.connect(host='localhost', user = 'root', password = '54Dreamer1015')
cursor = conn.cursor()
sql = "SELECT sponsored, `condition`, count(best_offer),avg(best_offer), \
       avg(item_price), min(item_price), max(item_price), std(item_price),\
       avg(num_item), min(num_item), max(num_item), std(num_item), \
       avg(seller_score), min(seller_score), max(seller_score), std(seller_score), \
       avg(shipprc), min(shipprc), max(shipprc), std(shipprc), count(retpc), avg(retpc) \
       FROM ebay.ebay_items \
       group by sponsored, `condition`"
# Use all the SQL you like
cursor.execute(sql)
rows = cursor.fetchall()
results = list(rows)
stats = pd.DataFrame(results, columns =['sponsored', 'condition', 'best_offer_count', 'best_offer_mean',
                                        'item_price_avg', 'item_price_min', 'item_price_max', 'item_price_std',
                                        'num_item_avg', 'num_item_min', 'num_item_max','num_item_std',
                                        'seller_score_avg', 'seller_score_min','seller_score_max', 'seller_score_std',
                                        'shipprc_avg', 'shipprc_min', 'shipprc_max', 'shipprc_std', 
                                        'retpc_count', 'retpc_mean']) 


conn.commit()
cursor.close() 
stats.head()

# Summary from the dataframe
#df = pd.DataFrame(final_results)
#df.groupby(['sponsored','condition']).agg({'best_offer':['count','mean']\
#                                           ,'item_price':['mean','min','max','std']\
#                                           ,'num_item':['mean','min','max']\
#                                           ,'seller_score':['mean','min','max']\
#                                           ,'shipprc':['mean','min','max']\
#                                           ,'retpc':['count','mean']})

stats

"""1) For item price in general, sponsored items are more expensive than nonsponsored in each condition category. Especially for those Manufacturer refurbished items, mean price of sponsored items is about 47% higher than nonsponsored ones.

2) For items sold, sponsored items also perform better than nonsponsored ones excpet for open box category. The reason might be the sponsored ones will mostly be put in the front pages that are most available to customers.

3) What reflected from seller_score is interesting, as nonsponsored items almost outperform over sponsored ones excpet for new items.

4) As for return policy, we could tell the sponsored items are generally uner more flexible policy as more than half of the items are returnable.

In short, no single variable could be used to predict the item is sponsored or not. But combined with several variables, including condition, item price, number of items sold and return policy could make the prediction more precise.
"""